{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Change these variables to your desired values\n",
    "img_height = 256\n",
    "img_width = 256\n",
    "img_channels = 1\n",
    "\n",
    "path_videos = 'assets/video_data_3/'\n",
    "path_out_images = 'assets/labeled_photos_3(cropped)/'\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "label_names=['Human',\n",
    "             'Interaction frontal',\n",
    "             'Interaction lateral', \n",
    "             'Interaction vertical',\n",
    "             'Crowded', \n",
    "             'Drink',\n",
    "             'Curiosity', \n",
    "             'Queue',\n",
    "             'Low visibility', \n",
    "             'Nothing']\n",
    "layer_names = ['conv2d_1', \n",
    "               'conv2d_2', \n",
    "               'conv2d_3', \n",
    "               'conv2d_4', \n",
    "               'conv2d_5', \n",
    "               'conv2d_6', \n",
    "               'conv2d_7']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.applications.vgg16 import VGG16\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_frame(frame):\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY) # convert to greyscale\n",
    "    frame = cv2.resize (frame, (img_width, img_height), interpolation=cv2.INTER_CUBIC) # rezize\n",
    "    return frame\n",
    "\n",
    "def frame_from_pointer (path, video, frame, verbose=False):\n",
    "    if verbose: print ('In '+path+'/'+str(video)+'.mp4' + ', taking frame ' + str(frame))\n",
    "    cap = cv2.VideoCapture(path+'/'+str(video)+'.mp4')\n",
    "    if (cap.isOpened()== False):\n",
    "        print(\"Error opening video file\") \n",
    "        return -1\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, frame-1)\n",
    "    ret, frame = cap.read() # Capture next frame\n",
    "    if ret==True:\n",
    "        cap.release()\n",
    "        return frame\n",
    "    else:\n",
    "        print(\"Error opening frame\") \n",
    "        return -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('assets/models/STABLE_SR2_november29.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grad-CAM\n",
    "I'm using Keras-vis library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "from vis.utils import utils\n",
    "import vis \n",
    "from vis.visualization import visualize_cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_map(grads, _img, label=\"undefined\", layer=\"last\"):\n",
    "    fig, axes = plt.subplots(1,2,figsize=(14,5))\n",
    "    axes[0].imshow(_img)\n",
    "    axes[1].imshow(_img)\n",
    "    i = axes[1].imshow(grads,cmap=\"jet\",alpha=0.8)\n",
    "    fig.colorbar(i)\n",
    "    plt.suptitle(\"Class = {} \\nLayer = {}\".format(\n",
    "                      label,\n",
    "                      layer))\n",
    "def preprocess_heatmap(grad_top, target_size):\n",
    "    # We resize the heatmap to have the same size as the original image\n",
    "    grad_top = cv2.resize(grad_top, target_size)\n",
    "\n",
    "    # Convert to Grayscale (following the keras-viz pattern)\n",
    "    heatmap = 2.0*grad_top[:,:,0] + 2.0*grad_top[:,:,1] - 1.0*grad_top[:,:,2]\n",
    "    heatmap += abs(min(heatmap.min(), 0))\n",
    "    if heatmap.max(): heatmap /= heatmap.max()\n",
    "    heatmap *= 255\n",
    "    heatmap = heatmap.astype('uint8')\n",
    "    \n",
    "    return heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_idx = utils.find_layer_idx(model, 'dense_3')\n",
    "# Swap softmax with linear\n",
    "model.layers[layer_idx].activation = keras.activations.linear\n",
    "model = utils.apply_modifications(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_original = frame_from_pointer('assets/video_data_3',1569153540, 6229, verbose=True)\n",
    "plt.imshow(frame_original)\n",
    "\n",
    "frame = preprocess_frame (frame_original)\n",
    "frame = frame.reshape(frame.shape[0], frame.shape[1], img_channels)\n",
    "frame = np.expand_dims(frame, axis=0)\n",
    "\n",
    "y_hat = model.predict(frame)\n",
    "print(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_idx = 2\n",
    "\n",
    "layer = layer_names[-3]\n",
    "grad_top3  = visualize_cam(model, layer_idx, class_idx, \n",
    "                           seed_input = frame,\n",
    "                           penultimate_layer_idx = utils.find_layer_idx(model, layer),\n",
    "                           backprop_modifier     = None,\n",
    "                           grad_modifier         = None)\n",
    "plot_map(grad_top3, frame.reshape(frame.shape[1], frame.shape[2]), label_names[class_idx], layer)\n",
    "\n",
    "layer = layer_names[-2]\n",
    "grad_top2  = visualize_cam(model, layer_idx, class_idx, \n",
    "                           seed_input = frame,\n",
    "                           penultimate_layer_idx = utils.find_layer_idx(model, layer),\n",
    "                           backprop_modifier     = None,\n",
    "                           grad_modifier         = None)\n",
    "plot_map(grad_top2, frame.reshape(frame.shape[1], frame.shape[2]), label_names[class_idx], layer)\n",
    "\n",
    "layer = layer_names[-1]\n",
    "grad_top1  = visualize_cam(model, layer_idx, class_idx, \n",
    "                           seed_input = frame,\n",
    "                           penultimate_layer_idx = utils.find_layer_idx(model, layer),\n",
    "                           backprop_modifier     = None,\n",
    "                           grad_modifier         = None)\n",
    "plot_map(grad_top1, frame.reshape(frame.shape[1], frame.shape[2]), label_names[class_idx], layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vizualizing every filter\n",
    "Reference: https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/5.4-visualizing-what-convnets-learn.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "\n",
    "# Extracts the outputs of the top 8 layers:\n",
    "layer_outputs = [layer.output for layer in model.layers[:14]]\n",
    "# Creates a model that will return these outputs, given the model input:\n",
    "activation_model = models.Model(inputs=model.input, outputs=layer_outputs)\n",
    "\n",
    "# This will return a list of 5 Numpy arrays:\n",
    "# one array per layer activation\n",
    "activations = activation_model.predict(frame)\n",
    "len(activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# These are the names of the layers, so can have them as part of our plot\n",
    "layer_names = []\n",
    "for layer in model.layers[:14]:\n",
    "    layer_names.append(layer.name)\n",
    "\n",
    "images_per_row = 16\n",
    "\n",
    "# Now let's display our feature maps\n",
    "for layer_name, layer_activation in zip(layer_names, activations):\n",
    "    # This is the number of features in the feature map\n",
    "    n_features = layer_activation.shape[-1]\n",
    "\n",
    "    # The feature map has shape (1, size, size, n_features)\n",
    "    size = layer_activation.shape[1]\n",
    "\n",
    "    # We will tile the activation channels in this matrix\n",
    "    n_cols = n_features // images_per_row\n",
    "    display_grid = np.zeros((size * n_cols, images_per_row * size))\n",
    "\n",
    "    # We'll tile each filter into this big horizontal grid\n",
    "    for col in range(n_cols):\n",
    "        for row in range(images_per_row):\n",
    "            channel_image = layer_activation[0,\n",
    "                                             :, :,\n",
    "                                             col * images_per_row + row]\n",
    "            # Post-process the feature to make it visually palatable\n",
    "            channel_image -= channel_image.mean()\n",
    "            channel_image /= channel_image.std()\n",
    "            channel_image *= 64\n",
    "            channel_image += 128\n",
    "            channel_image = np.clip(channel_image, 0, 255).astype('uint8')\n",
    "            display_grid[col * size : (col + 1) * size,\n",
    "                         row * size : (row + 1) * size] = channel_image\n",
    "\n",
    "    # Display the grid\n",
    "    scale = 1. / size\n",
    "    plt.figure(figsize=(scale * display_grid.shape[1],\n",
    "                        scale * display_grid.shape[0]))\n",
    "    plt.title(layer_name)\n",
    "    plt.grid(False)\n",
    "    plt.imshow(display_grid, aspect='auto', cmap='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "License: Creative Commons 4.0 Attribute"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
